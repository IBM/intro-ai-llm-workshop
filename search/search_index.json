{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#ai-and-windows-granite-workshop","title":"AI and Windows Granite Workshop","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced IBM Granite AI foundation models for a number of use cases that demonstrates the value of generative AI.</p> <p>The goals of this workshop are:</p> <ul> <li>Installing Open Source AI tools on your own Windows 11 laptop (Mac instructions to follow soon) to run AI locally.</li> <li>Running LLM models via command line</li> <li>Running LLM models with a UI interface</li> <li>Try to improve results with Retrieval-Augmented Generation </li> <li>Running with a multimodal LLM </li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Agenda</li> <li>Compatibility</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Pre-Work Prerequisites for the workshop Lab 1: Command-Line Ollama Lab 1 Lab 2: Using Open-WebUI Interface Lab 2 Lab 3: Graphics and MultiModal LLM Lab 3 Lab 4: Retrieval-Augmented Generation Lab 4 Lab 5: Agentic AI with Open-WebUI Lab 5"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>Windows 11 (Ideally with at least 16GB of memory, but 8GB memory can work with smaller models.</li> <li>(MacOS instructions included)</li> </ul>"},{"location":"#technology-used","title":"Technology Used","text":"<ul> <li>IBM Granite Models</li> <li>Open Source Ollama</li> <li>Open source Open-WebUI</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>James Busche</li> <li>Yi-Hong Wang</li> </ul>"},{"location":"lab-1/","title":"Command-Line Ollama","text":"<p>Ollama provides a CLI tool to interact with large language models. In this lab, you will learn how to use it to run the IBM granite models and get answers to some questions.</p> <p>You will need the Windows <code>Command Prompt</code> or the <code>Terminal</code> in the MacOS to run the Ollama CLI in this lab.</p>"},{"location":"lab-1/#list-the-available-models-downloaded","title":"List the available models downloaded","text":"<p>Use the following command to list the models in your current system. <pre><code>ollama list\n</code></pre></p>"},{"location":"lab-1/#try-granite332b-model","title":"Try <code>granite3.3:2b</code> Model","text":"<ol> <li> <p>In a CMD prompt or terminal window, use <code>ollama run</code> command and specify the model name to run a model and start the     conversation. Let's try out the <code>granite3.3:2b</code> model.:     <pre><code>ollama run granite3.3:2b\n</code></pre>     You will see the prompt like this:     <pre><code>&gt;&gt;&gt; Send a message (/? for help)\n</code></pre></p> </li> <li> <p>Ask some questions to the model. Here are some examples:     <pre><code>What is the difference between a mammal and a reptile? Provide examples for each.\n</code></pre> <pre><code>Create an easy to make chocolate chip cookie recipe\n</code></pre> <pre><code>Who was Marie Curie and what are her key discoveries in the field of chemistry and physics?\n</code></pre>     Hit <code>&lt;Enter&gt;</code> key when you finish your question and you will see the model start to reply your question. For example:     <pre><code>&gt;&gt;&gt; What is the difference between a mammal and a reptile? Provide examples for each.\nMammals and reptiles are both distinct groups of animals within the animal kingdom, differing in several key characteristics, including their body coverings, reproduction strategies, and warm-bloodedness. Here's a detailed comparison with examples for each:\n\n1. Body Covering:\n- Mammals: They have smooth, hairy, or scaly skin (depending on the species). Their young are born alive and nourished by milk through mammary glands. Examples include humans, dogs, cats, cows, and dolphins.\n- Reptiles: They are covered with scales for protection and water conservation. Their young hatch from eggs, unlike mammals that give live birth or lay eggs like birds. Examples include snakes, lizards, turtles, and crocodiles.\n\n2. Reproduction Strategy:\n- Mammals: Most mammals exhibit internal fertilization (egg within the parent's body) and nurse their young with milk. Examples include humans (milk-producing mammary glands), pigs, horses, and whales.\n- Reptiles: Typically lay eggs externally, and the development of embryos occurs without nourishment from a mother's body. Examples include sea turtles, alligators, and most lizards.\n\n3. Warm-bloodedness (Endothermy):\n- Mammals: They maintain a stable internal body temperature regardless of external conditions through thermogenesis (heat production). Examples include humans, dogs, cats, and polar bears.\n- Reptiles: Most reptiles are ectothermic, meaning they rely on external sources like sunlight to regulate their body temperature. This allows them to adapt to diverse habitats but makes them less efficient in colder environments. Examples include crocodiles, snakes,\nand turtles.\n\n4. Respiratory System:\n- Mammals: They possess lungs for gas exchange and use highly vascularized (well-developed network of blood vessels) systems for efficient oxygen uptake and carbon dioxide removal. Examples include humans, dogs, and rodents.\n- Reptiles: Some reptiles have lungs, while others employ other respiratory methods like skin respiration (exchange of gases through the skin). Crocodilians, for instance, rely on cutaneous (skin) respiration, allowing them to survive in oxygen-poor environments.\n\nIn summary, mammals and reptiles differ significantly in their body coverings, reproductive strategies, thermal regulation, and respiratory systems. Mammals are warm-blooded, give birth to live young or lay eggs with mammary glands, and have sophisticated internal\norgans for efficient gas exchange. Reptiles, on the other hand, are ectothermic, lay eggs, and employ various respiratory methods adapted to their habitats.\n</code></pre></p> </li> <li> <p>After you receive the response or answer from the model, you can ask the model to explain or refine the answer if you have further questions     which are related to the first question or the response content. This is the concept of the <code>context</code>.</p> </li> <li> <p>To clear the context, you can use the <code>/clear</code> command. Then you can start a new conversation.     <pre><code>/clear\n</code></pre>     You will see the following message indecating the context is cleared:     <pre><code>Cleared session context\n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre></p> </li> <li> <p>Instructions for changing the behavior of the model:</p> <p>Give it clear instructions to change it's behavior.  For example: <pre><code>Answer questions from now on in Spanish\n</code></pre> <pre><code>Answer questions like a pirate\n</code></pre></p> </li> <li> <p>Exit out of the current model     <pre><code>/bye\n</code></pre></p> </li> </ol>"},{"location":"lab-1/#try-another-model","title":"Try Another Model","text":"<p>Let's try out another model with more questions:</p> <ol> <li> <p>List the available models in the system again:     <pre><code>ollama list\n</code></pre></p> </li> <li> <p>How about <code>granite3.2-version:latest</code> model:     <pre><code>ollama run granite3.2-vision:latest\n</code></pre></p> </li> <li>Some example questions:     <pre><code>Create an easy to make chocolate chip cookie recipe\n</code></pre> <pre><code>List the ingredients as well as 5 optional additions\n</code></pre></li> <li>Exit out of the current model:     <pre><code>/bye\n</code></pre></li> </ol>"},{"location":"lab-2/","title":"Using Open-WebUI Interface","text":"<p>Open-WebUI offers a user-friendly web interface for interacting with models, along with other features. In this lab, you will learn how to use it and explore some of its basic features.</p>"},{"location":"lab-2/#preparation","title":"Preparation","text":"<p>Open-WebUI is a Python package installed in the Python virtual environment you created earlier. Activate your Python virtual environment if you haven't already:</p> WindowsMacOS <ol> <li>In a command prompt, run the go-webui.bat file you created in pre-work: <pre><code>go-webui.bat\n</code></pre></li> </ol> <ol> <li>In a command terminal, run the go-webui.sh file you created in pre-work: <pre><code>go-webui.sh\n</code></pre></li> </ol>"},{"location":"lab-2/#start-the-open-webui","title":"Start the Open WebUI","text":"<p>In a CMD prompt or terminal window, issue the command: <pre><code>open-webui serve\n</code></pre></p>"},{"location":"lab-2/#access-the-open-webui-web-interface","title":"Access the Open WebUI Web Interface","text":"<p>By default, Open WebUI serves on the port 8080. Click the URL below to access the Open WebUI interface: http://localhost:8080</p> <p>Click on the <code>Get Started</code> button at the bottom of the page and follow the instructions to create the admin account for the first time. Then you can use the account to log in to the Open WebUI afterwards.</p>"},{"location":"lab-2/#select-a-model","title":"Select a Model","text":"<p>By default, Open WebUI connects to the Ollama server in your local system. Therefore, it uses the available models on your Ollama server. Click on the model name on the top left next to the <code>New Chat</code> button to switch to a different model. See the highlight of the image below for more details: </p> <p>Let's select <code>granite3.3:8b</code> for this lab. <pre><code>granite3.3:8b\n</code></pre></p>"},{"location":"lab-2/#new-chat","title":"New Chat:","text":"<p>Start a new chat and type the question in the highlighted box: </p> <p>Here are some example questions you can ask: <pre><code>Describe the water cycle in detail, mentioning evaporation, condensation, precipitation, and collection.\n</code></pre> <pre><code>Who was Ada Lovelace, and why is she considered a pioneer in computer science?\n</code></pre> <pre><code>Why are cats better companions than dogs?\n</code></pre> <pre><code>explain your reasoning for why cats and dogs are not inherently better\n</code></pre></p> <p>Hit the <code>&lt;Enter&gt;</code> key or the up-arrow button to submit the question and wait for the response from the model.</p>"},{"location":"lab-2/#context","title":"Context","text":"<p>Within the same context, you can ask a follow-up question or provide additional instructions to adjust how the model answers your question. For example, give it clear instructions to change the behavior: <pre><code>Only answer questions in rhyme. What was domesticated first, a dog or a cat?\n</code></pre></p>"},{"location":"lab-2/#clear-the-context","title":"Clear the context","text":"<p>Initiate a new chat by clicking on \"New Chat\" on the top left corner of the page: </p>"},{"location":"lab-3/","title":"Graphics and MultiModal LLM","text":""},{"location":"lab-3/#31-continuing-to-use-the-running-open-webui-with-your-browser-at","title":"3.1 Continuing to use the running Open-WebUI with your browser at:","text":"<p>http://localhost:8080</p>"},{"location":"lab-3/#32-pick-the-vision-model-to-use-for-example","title":"3.2 Pick the vision model to use, for example:","text":"<pre><code>    granite3.2-vision:2b\n</code></pre>"},{"location":"lab-3/#33-this-small-vision-model-is-multi-modal","title":"3.3 This small vision model is multi-modal.","text":"<p>Multi-modal means the model has been trained from multiple modalities of data, such as text, images, audio, and video. So while the primary use of the vision model is to understand visual content, it's also trained with language datasets.  So you can issue it questions and commands and it will still respond as a small language model.  </p> <p>For example, you can ask a few simple questions: <pre><code>Why would hydrogen gas not be preferred in balloons?\n</code></pre> <pre><code>Create an easy apple pie recipe for the upcoming holiday?\n</code></pre></p>"},{"location":"lab-3/#34-clear-the-context-by-clicking-on-new-chat","title":"3.4 Clear the context by clicking on \"New Chat\"","text":""},{"location":"lab-3/#35-using-another-tab-on-the-browser-search-for-a-image-for-example","title":"3.5 Using another tab on the browser, search for a image.  For example:","text":"<pre><code>Kangaroo\n</code></pre>"},{"location":"lab-3/#36-in-the-search-results-click-on-the-images-tab-and-right-click-on-one-of-the-photos-you-like-and-select-copy-image","title":"3.6 In the search results, click on the \"Images\" tab and right-click on one of the photos you like and select \"Copy Image\"","text":""},{"location":"lab-3/#37-using-the-open-webui-browser-do-a-ctrl-v-to-paste-the-copied-image-to-the-open-webui-and-press-enter-the-granite-vision-model-will-summarize-what-it-sees-from-the-image","title":"3.7 Using the Open-WebUI browser, do a <code>CTRL-V</code> to paste the copied image to the Open-WebUI and press enter. The Granite vision model will summarize what it sees from the image.","text":""},{"location":"lab-3/#38-go-ahead-and-ask-the-ai-more-questions-about-the-animal-in-the-image-for-example","title":"3.8 Go ahead and ask the AI more questions about the animal in the image.  For example:","text":"<pre><code>What is the lifespan for the animal in the photo?\n</code></pre>"},{"location":"lab-3/#39-keep-learning","title":"3.9 Keep learning!","text":"<p>If you have additional time and interest, you can continue to try out what you've learned about using Open Source AI tools to run your own local AI system.</p> <p>Have fun!</p>"},{"location":"lab-4/","title":"Retrieval-Augmented Generation","text":""},{"location":"lab-4/#41-in-the-open-webui-switch-to-the-smallest-granite-model-granite332b","title":"4.1 In the Open-WebUI, switch to the smallest Granite model: granite3.3:2b","text":"<pre><code>granite3.3:2b\n</code></pre>"},{"location":"lab-4/#42-click-on-new-chat-so-the-context-is-clear","title":"4.2 Click on \"New Chat\" so the context is clear","text":""},{"location":"lab-4/#43-ask-it-for","title":"4.3 Ask it for:","text":"<p><pre><code>List the past and current CEOs of the IBM corporation from first to last.\n</code></pre> The answer isn't correct.</p>"},{"location":"lab-4/#44-click-on-new-chat-to-clear-the-context","title":"4.4 Click on \"New Chat\" to clear the context","text":""},{"location":"lab-4/#45-download-a-small-text-file-with-the-correct-list-of-ibm-ceos-to-your-downloads-folder","title":"4.5 Download a small text file with the correct list of IBM CEOs to your Downloads folder:","text":"<p>IBM.txt</p>"},{"location":"lab-4/#46-click-on-the-and-then-upload-files","title":"4.6 Click on the \"+\" and then Upload files","text":"<p>Upload the IBM.txt file that you just downloaded</p>"},{"location":"lab-4/#47-now-ask-it-our-question-about-the-ceos-of-ibm","title":"4.7 Now ask it our question about the CEOs of IBM:","text":"<p><pre><code>List all the past and current CEOs of the IBM corporation from first to last.\n</code></pre> The answer should now be correct. (For example, always before it forgets Jon Akers)</p>"},{"location":"lab-4/#48-we-can-also-find-and-download-information-to-pdf-from-wikipedia","title":"4.8 We can also find and download information to pdf from Wikipedia:","text":"<p>For example: History of IBM</p>"},{"location":"lab-4/#49-on-the-right-click-on-tools-and-click-on-download-as-pdf","title":"4.9  On the right, click on \"Tools\" and Click on \"Download as PDF\"","text":""},{"location":"lab-4/#410-then-use-this-history_of_ibmpdf-as-a-rag-by-clicking-on-the-and-adding-history_of_ibmpdf-as-a-file","title":"4.10 Then use this History_of_IBM.pdf as a RAG by clicking on the + and adding \"History_of_IBM.pdf\" as a file.","text":""},{"location":"lab-4/#411-then-use-the-open-webui-to-ask-more-questions-about-ibm","title":"4.11 Then use the Open-WebUI to ask more questions about IBM.","text":"<p><pre><code>Who is the first female CEO of IBM?\n</code></pre> The answer should be Ginni Rometty</p>"},{"location":"lab-5/","title":"Agentic AI with Open-WebUI","text":"<p>LLM Models are only as good as the data they're trained on.  But what if you want current information from the web? The LLM model could be 6-months or a year behind on current events or weather, sport team results, etc. As you saw in lab-4, one solution is to add a RAG document with the latest data/information you're interested in, and it'll get used in the answer. But another way is to use Agentic AI where the AI solution could reach out to the web to do a web crawl and get the latest information for you.  To do that, we need the following:</p> <p>5.1 We need these models downloaded to your Ollama server: <pre><code>ollama pull granite3.3:8b\nollama pull granite3.3:2b\nollama pull granite-embedding:278m\n</code></pre></p> <p>5.2 Make sure you're started up and are in your python environment:</p> WindowsMacOS <ol> <li>From a command prompt, run the following command to activate your virtual environment:    <pre><code>\"venv/Scripts/activate.bat\"\n</code></pre></li> </ol> <ol> <li>From a terminal window, run the following command to activate your virtual environment:    <pre><code>source .venv/bin/activate\n</code></pre></li> </ol> <p>5.3 In your python environment, we need to install these 3 python packages: <pre><code>pip install gpt-researcher\npip install langchain_ollama\npip install duckduckgo_search \n</code></pre></p> <p>5.4 In your Open-WebUI browser window:</p> <p>5.4.1 Click on the circle with your initial on the far right of the screen and select \"Admin Panel.\"</p> <p>5.4.2 Click on the \"Functions\" tab at the top</p> <p>5.4.3 Click on the \"+\" button to create an Search Agent function</p> <p>5.4.4 Give it a name:  For example: Searcher</p> <p>5.4.5 Give it a Description:  For example: Searcher-AI</p> <p>5.4.6 Paste in this code:</p> <pre><code>\"\"\"\ntitle: Researcher Agent Pipe V2\nauthor: yihongwang@gmail.com\ndate: 08-01-2025\nversion: 0.2\nlicense: MIT\ndescription: Enable the GPT Researcher using Granite models via Ollama by default.\n             Modify valve settings if you use different models/API.\n             Be sure to update the `OLLAMA_BASE_URL` to point to your Ollama server.\nrequirements: gpt-researcher, langchain_ollama, duckduckgo-search\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nimport asyncio\nimport json\nimport os\nimport sys\nfrom gpt_researcher import GPTResearcher\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        FAST_LLM: str = Field(default=\"ollama:granite3.3:2b\")\n        SMART_LLM: str = Field(default=\"ollama:granite3.3:2b\")\n        STRATEGIC_LLM: str = Field(default=\"ollama:granite3.3:2b\")\n        EMBEDDING: str = Field(default=\"ollama:granite-embedding:278m\")\n        OLLAMA_BASE_URL: str = Field(default=\"http://localhost:11434\")\n        OPENAI_API_KEY: str = Field(default=\"ollama\")\n\n    def __init__(self):\n        self.valves = self.Valves()\n\n    async def pipe(self, body: dict):\n        print(body[\"messages\"][-1][\"content\"])\n        os.environ[\"FAST_LLM\"] = self.valves.FAST_LLM\n        os.environ[\"SMART_LLM\"] = self.valves.SMART_LLM\n        os.environ[\"STRATEGIC_LLM\"] = self.valves.STRATEGIC_LLM\n        os.environ[\"EMBEDDING\"] = self.valves.EMBEDDING\n        os.environ[\"OPENAI_API_KEY\"] = self.valves.OPENAI_API_KEY\n        os.environ[\"OLLAMA_BASE_URL\"] = self.valves.OLLAMA_BASE_URL\n        os.environ[\"RETRIEVER\"] = \"duckduckgo\"\n        os.environ[\"LLM_KWARGS\"] = json.dumps({\"num_ctx\": 1024 * 128})\n        os.environ[\"MAX_ITERATIONS\"] = \"1\"\n        os.environ[\"MAX_SUBTOPICS\"] = \"1\"\n        os.environ[\"DEEP_RESEARCH_BREADTH\"] = \"1\"\n        os.environ[\"DEEP_RESEARCH_DEPTH\"] = \"1\"\n        os.environ[\"DEEP_RESEARCH_CONCURRENCY\"] = \"1\"\n        os.environ[\"REASONING_EFFORT\"] = \"low\"\n        os.environ[\"FAST_TOKEN_LIMIT\"] = \"1000\"\n        os.environ[\"SMART_TOKEN_LIMIT\"] = \"1000\"\n        os.environ[\"STRATEGIC_TOKEN_LIMIT\"] = \"1000\"\n        os.environ[\"BROWSE_CHUNK_MAX_LENGTH\"] = \"1024\"\n        os.environ[\"MAX_SEARCH_RESULTS_PER_QUERY\"] = \"1024\"\n        os.environ[\"TOTAL_WORDS\"] = \"300\"\n        researcher = GPTResearcher(\n            query=body[\"messages\"][-1][\"content\"],\n            report_type=\"research_report\",\n        )\n        research_result = await researcher.conduct_research()\n        report = await researcher.write_report()\n\n        # Get additional information\n        research_context = researcher.get_research_context()\n        research_costs = researcher.get_costs()\n        research_images = researcher.get_research_images()\n        research_sources = researcher.get_research_sources()\n        print(\"Report:\")\n        print(report)\n        print(\"\\nResearch context:\")\n        print(research_context)\n        print(\"\\nResearch Costs:\")\n        print(research_costs)\n        print(\"\\nNumber of Research Images:\")\n        print(len(research_images))\n        print(\"\\nNumber of Research Sources:\")\n        print(len(research_sources))\n        return report\n</code></pre> <p>Note</p> <p>Note: We've reduced many of the GPT Researcher environment settings to much smaller than default so that the lab will run faster, ideally under 5 minutes.  Feel free to change the GPT Researcher environmental later for longer-running but deeper researched answers. More info on the GPT Researcher settings can be found at: GPT Researcher</p>"},{"location":"lab-5/#55-on-the-right-youll-see-a-toggle-switch-youll-want-to-make-your-new-function-green-enabled","title":"5.5 On the right you'll see a toggle switch - you'll want to make your new function green (Enabled.","text":""},{"location":"lab-5/#56-in-the-open-webui-switch-to-use-your-searcher-function-rather-than-one-of-the-ollama-models","title":"5.6 In the Open-WebUI, switch to use your \"Searcher\" function rather than one of the Ollama models.","text":""},{"location":"lab-5/#57-ask-your-question-for-example","title":"5.7 Ask your question, for example:","text":"<pre><code>Tell me the weather today in San Jose California\n\nor\n\nWhat is the last result of the San Franicisco Giants game?\n</code></pre> <p>After about 2-5 minutes, it should have scraped the result from the web and presented it in the Open-WebUI.</p>"},{"location":"pre-work/","title":"Pre-work","text":"<p>Assumptions:</p> <p>These labs can be run on a Windows 11 or MacOS machine. Click on the operation system you are using below to get the corresponding instructions.</p>"},{"location":"pre-work/#ollama-installation","title":"Ollama Installation","text":"WindowsMacOS <ol> <li> <p>Download Ollama for Windows</p> </li> <li> <p>Install Ollama by double-clicking on the downloaded <code>OllamaSetup.exe</code>     And then clicking on \"Install\"</p> </li> <li> <p>In a CMD or terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.  It will take time to download the bigger models depending on your network speed. <pre><code>ollama pull granite3.3:2b\nollama pull granite3.2-vision:2b\nollama pull granite3.3:8b\n</code></pre></p> </li> </ol> <p>Note: We won't use them for this lab, but if you wanted to try some other LLM models, you can start with: <pre><code>ollama pull deepseek-r1:1.5b\nollama pull llama3.2:3b\n</code></pre> Check out the entire Hugging face library of models at: https://ollama.com/library</p> <ol> <li> <p>Download Ollama for Mac</p> </li> <li> <p>Drag the Ollama to your applications folder, then double-click on the ollama application to start it up.</p> </li> <li> <p>In a terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.  It will take time to download the bigger models depending on your network speed.     <pre><code>ollama pull granite3.3:2b\nollama pull granite3.2-vision:2b\nollama pull granite3.3:8b\n</code></pre></p> <p>Note: We won't use them for this lab, but if you wanted to try some other LLM models, you can start with: <pre><code>ollama pull deepseek-r1:1.5b\nollama pull llama3.2:3b\n</code></pre> Check out the entire Hugging face library of models at: https://ollama.com/library</p> </li> </ol>"},{"location":"pre-work/#install-python","title":"Install Python","text":"<p>For the workshop you will need Python 3.11.9. Follow the steps below for your operating system:</p> WindowsMacOS <ol> <li> <p>Download the Python python-3.11.9-amd64.exe</p> </li> <li> <p>Double-click to install it</p> <p>Note</p> <p>While installing - make sure you check the box to \"add the python.exe to path\"**</p> </li> <li> <p>Close and re-open your command prompt and then check to make sure python is findable on the path by typing:     <pre><code>python --version\n</code></pre>     It should return: \"Python 3.11.9\"</p> </li> </ol> <p>Note</p> <p>If it doesn't find the python command, likely you forgot to check the box to put python in the path.  No worry, just:</p> <ol> <li>Double-click on the downloaded python-3.1.9-amd64.exe file again and this time chose \"Modify\" and click \"next\" </li> <li>Put a check next to the box: \"Add Python to the environnment variables\" and click \"Install\".</li> </ol> <ol> <li> <p>Optionally, you can also download and install git from git-scm.com/downloads </p> <ol> <li>The latest one as of writing this document is: Git-2.49.0-64-bit.exe</li> <li>Double-click on the downloaded Git-2.49.0-64-bit.exe, take all the defaults and install</li> </ol> </li> </ol> <ol> <li> <p>Open a terminal window and check to see if you have the brew command already:     <pre><code>brew\n</code></pre></p> </li> <li> <p>If you have Brew, skip to the next step.  Otherwise, install brew on MacOS with this command:     <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre></p> </li> <li> <p>Install uv for MacOS. If you don't have them already:     <pre><code>brew update\nbrew install uv\n</code></pre></p> </li> <li> <p>Install Python 3.11.9 for MacOS     <pre><code>uv venv --python 3.11 --seed .venv\n</code></pre></p> </li> <li> <p>Activate the Python virtual env you just created:     <pre><code>source .venv/bin/activate\n</code></pre></p> </li> <li> <p>Verify the Python version:     <pre><code>python --version\n</code></pre>     It should return now 3.11.x</p> </li> </ol>"},{"location":"pre-work/#python-virtual-environment","title":"Python Virtual Environment:","text":"<p>Python virtual environment is used to isolated Python installations. Let's create one called \"venv\" and use it for the workshop.</p> WindowsMacOS <ol> <li> <p>Open the command prompt run the following command to create a new virtual environment:     <pre><code>python -m venv --upgrade-deps --clear venv\n</code></pre></p> </li> <li> <p>Activate your python environment by typing:     <pre><code>\"venv/Scripts/activate.bat\"\n</code></pre></p> </li> </ol> <ol> <li> <p>Open up a terminal window and install Python 3.11.x for MacOS which also creates a python virtual environment     <pre><code>uv venv --python 3.11 --seed .venv\n</code></pre></p> </li> <li> <p>Activate the Python virtual env you just created:     <pre><code>source .venv/bin/activate\n</code></pre></p> </li> <li> <p>Verify the Python version:     <pre><code>python --version\n</code></pre>     It should return now 3.11.x</p> </li> </ol>"},{"location":"pre-work/#install-the-open-webui","title":"Install the Open-WebUI","text":"WindowsMacOS <ol> <li>In the open command prompt, run the following command to Install the Open-WebUI:     <pre><code>pip install open-webui\n</code></pre></li> </ol> <ol> <li>In the open terminal window, run the following command to Install the Open-WebUI:     <pre><code>uv pip install open-webui \n</code></pre></li> </ol> <ol> <li>Start up open-webui. You will leave this running during the workshop.     <pre><code>open-webui serve\n</code></pre></li> </ol>"},{"location":"pre-work/#access-open-webui","title":"Access Open-WebUI","text":"<ol> <li> <p>Once the open-webui starts, use a browser to login to open-webui:     http://localhost:8080</p> </li> <li> <p>Enter a name, email address and password - these are stored locally. (Don't forget them!) </p> </li> </ol>"},{"location":"pre-work/#create-scripts-to-activate-and-start-your-environments-up-with-one-go-webui-script-going-forward","title":"Create scripts to activate and start your environments up with one \"go-webui\" script going forward:","text":"WindowsMacOS <ol> <li> <p>In a command prompt, type in \"notepad go-webui.bat\" and press enter, then add the following info and press save:     <pre><code>@echo off\nCALL \"%USERPROFILE%\\venv\\Scripts\\activate.bat\"\nopen-webui serve\n</code></pre></p> </li> <li> <p>Run the bat script from the command line in the future by typing:     <pre><code>go-webui.bat\n</code></pre></p> </li> </ol> <ol> <li>Open a terminal window and run the following commands to create a go-webui.sh shell script:     <pre><code>echo \"source .venv/bin/activate\" &gt; ~/Desktop/go-webui.command\necho \"open-webui serve\" &gt;&gt; ~/Desktop/go-webui.command\nchmod +x ~/Desktop/go-webui.command\n</code></pre></li> <li> <p>Run the shell script from a terminal window in the future by typing:     <pre><code>~/Desktop/go-webui.command/go-webui.command\n</code></pre></p> <p>Or you can doubleclick on the go-webui.command window on your Desktop</p> </li> </ol>"},{"location":"pre-work/#cleaning-up-after-the-lab-is-complete","title":"Cleaning up after the lab is complete:","text":"<ol> <li> <p>Remove all the models you don't want:     <pre><code>ollama list\nollama rm granite3.3:2b granite3.3:8b granite3.2-vision:2b\n</code></pre></p> </li> <li> <p>Uninstall ollama:</p> WindowsMacOS <p>In the search bar, search for Settings \u2192 Apps \u2192 Installed Apps and search for Ollama and click on Uninstall</p> <p>In a terminal, use <code>brew</code> to uninstall <code>ollama</code>: <pre><code>brew uninstall ollama\n</code></pre></p> <p>Or if you dragged Ollama to the applicatino folder, you can delete the Ollama application in the Application folder.</p> </li> <li> <p>Stop and remove open-webUI:</p> WindowsMacOS <ol> <li> <p>In the command line where open-webui is running, do a ctrl-C to stop the \"open-webui serve\"</p> </li> <li> <p>Deactivate your python environment session by typing:      <pre><code>venv/Scripts/deactivate.bat\n</code></pre></p> </li> <li> <p>If you want to remove the virtual environment completely:     <pre><code>rmdir /s venv\n</code></pre></p> </li> </ol> <ol> <li>In the command line where open-webui is running, do a ctrl-C to stop the \"open-webui serve\"</li> <li>Deactivate your python environment session by typing:      <pre><code>deactivate\n</code></pre></li> <li>If you want to remove the virutal environment completely:     <pre><code>rm -rf venv\n</code></pre></li> </ol> </li> </ol>"},{"location":"prerequisite/","title":"Prerequisite","text":"<p>This workshop uses ......</p> <p>Please follow the installation links below to set up your working environment:</p> <ul> <li>prerequisite 1</li> <li>prerequisite 2</li> </ul>"},{"location":"prerequisite/#clone-the-workshop-repository","title":"Clone the workshop repository","text":"<p>Various parts of this workshop will require the configuration files from the workshop repository. Use the following command to download the whole repository <pre><code>git clone https://github.com/IBM/intro-ai-llm-workshop.git\ncd intro-ai-llm-workshop\n</code></pre></p> <p>Or download the repository as a zip file, unzip it and get into the <code>intro-ai-llm-workshop-main</code> directory.</p>"},{"location":"resources/CONTRIBUTORS/","title":"Contributors","text":""},{"location":"resources/CONTRIBUTORS/#james-busche","title":"James Busche","text":"<ul> <li>Github: jbusche</li> <li>Github: yh-wang</li> </ul>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":""},{"location":"resources/RESOURCES/#ibm-granite-models","title":"IBM Granite Models","text":"<ul> <li>resource 1 Granite Workshop</li> <li>resource 2 IBM Granite AI foundation models</li> <li>resource 3 Granite Community</li> <li>resource 4 Granite Snack Cookbook</li> </ul>"}]}